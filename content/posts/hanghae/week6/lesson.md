+++
title = 'week6 강의 필기'
date = 2025-07-07T00:20:22+09:00
draft = true
+++

🟠WEEK6 강의

### 레디스 분산락
- 레디스: O(1). 메모리라서 빠름. file I/O보다 ~1000배.
내부구조 직관적. c++. 복잡한 오퍼레이션, 오버헤드 없음.
쿼리가 원자성 보장. SETNX. 키가 없을때만 쓴다.

충전, 사용 요청이 동시에 오면.
둘다 잔액 수정.

* 🔺스핀락을 보통 씀. 에러 던지기보다는 기다렸다가 - 
재시도 with delay.
🔺 wait time 길어도 되면 100ms, 실시간성 중요하면 10ms. 
wait time, max time
🔺 100ms, 총 10번 재시도 -> 1초까지만. (이후로는 첨부터 다시 요청하게끔)
- polling. 레디스에 많은 부하가 일어남. 
락을 얻을 수 있는 상태가 되면 우리한테 알려줬으면.
-> pub/sub 기능!!

* 🔺pub/sub
subscribe만 하면. 비동기적으로 알려줌.

* 분산락: 락을 직접 제어하는 것.
락을 획득, 반환하는 시기를 정확하게.
비즈니스 로직 중간에 획득하거나, 끝나기 전에 반환하면 안됨!

순서: 🔺🔺락 획득 후 트랜잭션해야함.
- 트랜잭션을 먼저 했을 때 가장 큰 문제는 락 대기가 걸렸을 때 DB 커넥션을 점유하게 된다는 것.
- 트랜잭션 커밋 전 락을 해제하게 되면, 다른애가 락을 들고 커밋 전의 값을 읽어가버림. 어쨌든 락의 목적은 커밋 전의 데이터를 누군가 읽지 못하게 하는것. 

'순서 보장'이라는 목적에서 보면
카프카도 쓸 수 있음.

* 카프카 메시징
동일한 메시지 = 같은 키를 가진. 발행한 순서 보장됨.

* 분산락은 가용성, 영속성 필수
레디스도 캐싱은 에러나도 좀 느릴 뿐 문제는 아닌데
🔺분산락은 Locking이라 치명적임. 
-> 같은 레디스라 하더라도 가용성, 영속성이 보장되어야함. 
=> 🔺'클러스터 형태'로 구성해 fail over 조치를 함. 

🔺가급적 managed 쓸 것. 
AWS elastic cache or 직접하면 클러스터로 구성.

* 분산락 예시.
동일한 키를 사용해야 lock이 작동.
하나의 키면 모든 유저가 lock 되어버림.
다른 유저끼리는 lock이 간섭할 필요 없음. 같은 유저에 대해 동시 충전될 때가 문제임.
-> userCache:{userId}

charge and pay 자체를 트랜잭션으로 묶을 때 🔺"db가 아닌 코드로서" 묶을 수 있음.
-> 분산락의 진짜 의미.

동시성 구현이라함은 테스트까지 확실히 해야함.
테스트코드 - 수백개 몰리는 경합상태 처리해보려면 K6.


### Caching

프레임워크 @어노테이션 쓰면 비즈니스로직-캐싱로직 분리할 수 있긴함.
애플리케이션 안에 캐싱로직이 있을 때 발생할 수 있는 문제?

대규모면 하나의 인스턴스로 대응하진 않을텐데. 
앞단 로드밸런서, 뒷단 여러대 인스턴스. 
서버1에서 캐싱했는데 서버2,3,4로 요청이 가는.. -> 캐시미스

캐시 데이터를 인스턴스 메모리에 직접 들고 서버끼리 동기화하기보다는 
🔺별도의 캐시 스토리지를 두자. WHY?
- 인스턴스당 RAM 4GB 정도 쓰는데, 캐시가 5GB면 메모리가 터질 수 있음..
- 분산환경 - ecs, 쿠버네티스, auto scaler가 계속 밸런싱을 해줘서 서버가 죽었다 켜질 때마다 캐시 데이터가 날라가게 됨. 

'레디스는 DB보다 훨씬 가볍다'.
db에 쿼리가 훨씬 덜 갈 수 있음.

레디스 서버는 하나에 메모리 100GB 박아넣어도 됨. 여러 애플리케이션이 공유하는거라 그만한 명분이 됨. 
+ secondary node. 🔺비휘발성인 것처럼. 메모리 보존 가능.

* application level 캐시와, external level 캐시를 혼용하는 경우도 많음.
- application level: 진짜 자주 사용되는 데이터. 네트워크 비용 없음. HTTP 요청 하나당 반드시 여러번 액세스되는 값 - 인증 등
- 분산캐시(external): 전체 메타데이터, 사용자 세션, 순위 등
여러 어플리케이션 인스턴스 간에 일관성 필요할 때
- 멀티 캐시 레이어: 기본적으로 레디스에 들고있는데 그중에 중요한건 레디스 인스턴스에도 캐싱해두는(중복). 어쨌든 레디스 인스턴스 부하도 덜 가게끔.
🔺e.g. DB 1테라 - 그중 중요한 10G는 외부 캐시가 들고있고 그 중에서 엄청 자주 쓰이는 데이터 100MB 정도는 각 애플리케이션 인스턴스가 들고있고. 

### 분산락과 캐싱의 혼용 Idea! 🔮
뉴스 사이트 - 광고 추천하는 컴포넌트.
주요 언론사들이 다 쓰는.
인기 뉴스 - 같은 리퀘가 엄청 들어옴. 인구학적 데이터로 연산하는데. AI 모델 쿼리. - 유저 세그먼트 분리해서 캐싱해두는데.
길면 1초 넘게 걸리는데, 1초에 몇백만개면 🔺캐싱 완료되기전에 AI 모델에 그대로 쌓여버림.
요청이 순차적으로 오면 앞에 하나가 캐싱에 넣고 두번째부터는 연산 save인데..
- AI 모델 터짐..
-> 🔺분산락으로 해결. 
🔺🔺어떤 캐시 데이터가 있는지 쿼리하는 작업부터, AI 모델 요청하고 캐싱하는 것 자체를 하나를 트랜잭션으로 보고 락으로 묶음. 
첫번째 요청이 캐시 리소스를 다룰 수 있는 권한(락)을 획득한 것. 
두번째 요청: 캐시를 확인해야하는데 락을 대기해야. 
=> 동시에 수백만건 요청이 와도 같은 요청이라면 AI 모델은 단 한번만 연산하면 되는 것. 


*** 과제 팁
- 내가 쓰는 서비스에 캐시가 어디에 사용되었을 지 상상해보기.
고려해야할 것들을 자연스럽게 얻어낼 수.
무신사 인기상품 캐싱. 수천가지 옷 중 full scan해서 줄세워야하는거라 . 몇 분 이상. => 미리 다 계산해놓은 것이 레디스든, db에 들어가든 캐싱임. 

분산락도 마찬가지.
- 예치금 충전과 동시에 구매가 일어나려면 하나의 트랜잭션으로 묶여야하고, 분산락으로 묶여야할 것. 

고민할 것 ***!! =======================================
- 🔺키를 잘 만들것. 
락을 제어하기 위해 락 리소스를 나타내는 키가 무엇인지.
키의 범위가 너무 넓지 않게. 
하나의 단위로 뭉쳐서 나가는거라, 🔺🔺내부에 데이터가 바뀌었을 때 캐시를 업데이트해주는 로직이 필요하고. 
유저에 적용되는 캐시 데이터도 방대하므로.

- 분산락: 락 안에 다른 작은 락들, db락들이 들어있는데.
내가 제어하고 있는 락이 무엇인지 알면서- 순서가 꼬이지 않아야함.
'락의 스코프'!

- 캐싱: 원격지에서 가져올 때 날리는 쿼리.
어떤 종류의 데이터를 주는지에 따라 캐시를 어떤식으로 저장할지가 달라짐.
- 캐시 적용이 불가한 것도 있음.
티켓 플랫폼은 대기열 -> 대기 번호를 받는 것은 캐싱할 수 없음. 같은 대기표를 여러 유저가 공유하지 않으니까. 구조적으로 캐시가 있을 수 없는.
- 언제 트래픽 부하를 줄일 수 있는지 판단. 아무데나 x.

인스턴스에도 부하가 많이 줄어드는지 CPU 등 체크하면 완벽. 

캐시가 더 중요하긴 함. 

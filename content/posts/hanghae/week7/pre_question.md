+++
title = 'week7 Q&A'
date = 2025-07-09T00:20:22+09:00
draft = true
+++

🟣🟣🟣WEEK7 Q&A
🥬🥬🥬
### 1. 아래 콘서트 예약 시스템의 비동기 설계가 적절할지 여쭤봅니다. 

sorted set을 두 개로 두고
waiting_queue
active_users
대기열 토큰 발급시 waiting_queue로 추가(예약을 위해 대기),
🔴active_users (예약페이지 진입)로 전환하기 위해 폴링을 하는것이 적절할까요?
폴링을 한다면 waiting_queue내 가장 먼저 들어온(타임스탬프가 가장 작은) 유저들을 100명(최대 동시접속자수)까지 전환할 수 있도록 할까 생각하고 있습니다.

기존에는 active user로의 전환을 BullMQ로 구현해두었습니다.
(대기열 토큰 발급시BullMQ에 add job, 
워커가 가져갈 때를 예약페이지 진입으로 간주하고 setTimeout 3분정도(예약시간 제한 가정) 후 job은 종료, 
예약하려면 처음부터 대기열 발급받는 방식)

이처럼 대기열 토큰시스템을 bullMQ로 할경우
워커 수만큼만 동시접속이 가능한 구조같아서 너무 확장성이 없는듯하여 고민이 있었는데, 
sorted set으로 관리할경우 
좌석 조회시 active_users sorted set에 해당 대기열토큰이 있는지만 확인하면 될거같아서 더 간단할 것 같긴하네요..
-> 🔴bullMQ 워커로 대기열토큰을 fifo로 가져가던 방식은 폐기하는게 맞을지,
아니면 실무에서 bullMQ와 같은 방식으로 대기열을 관리하기도 하는지 궁금합니다

-------------------------------------------------------------------

기존에 대기열 기능을 MQ를 통해 구성하셨군요 ㅎㅎ
질문 주신 내용은 BullMQ의 특징이라기 보다는 MQ로 대기열을 구현할 때 공통적으로 생길 수 있는 문제입니다.

MQ로 대기열을 구현하게 되면 보통 2가지 문제가 생깁니다.
일반적으로 MQ는 분산처리를 상정하고 설계가 되기 때문에 FIFO 기능을 지원하는 시스템이라도 기능이 아주 제한적이거나 관리 난이도가 올라가게 됩니다. (AWS의 SQS나 Kafka에서 FIFO를 보장하려면 여러가지 제약조건이 많이 붙게 된다는 사실이 아주 유명하죠.)
따라서 일부 상황에서만 FIFO가 보장되어야하는 종류의 작업이라면 MQ를 사용하는게 적절할 수 있겠습니다만, (배달앱에서 같은 동네의 주문 순서를 보장한다던지하는 식으로 FIFO가 보장되는 범위가 아주 좁은 경우) 대기열 같은 경우 아주 많은양의 이벤트(메시지)가 전부 FIFO가 보장되어야 하기 때문에 MQ의 설계 의도와 맞지 않는 부분이 있습니다.
전체 대기열의 규모를 관리하는게 쉽지 않습니다. 유저 입장에서는 내가 지금 들어와 있는 대기열에 몇명 정도가 같이 기다리고 있는지 보여주기를 기대하는데, (내 뒤에 몇명이 있는지는 몰라도 적어도 내 앞에는 몇명 정도 있는지는 보여줄 거라는 기대가 있죠.) MQ로 대기열을 구현하는 경우 이 정보를 다루기 위한 별도의 로직을 따로 구현해야한다는 문제가 있습니다.

위와 같은 문제로 인하여 실무적으로도 중규모정도의 예매 시스템이라면 Redis만을 이용하여 구현하는것이 일반적입니다. 동시에 수백만명을 받아야하는 대규모 예매 시스템이라면 MQ, Redis 등을 혼용하여 분산처리까지 고려해야하는 상황이 올 수도 있지만 이런 특수한 상황이라면 애초에 정합성을 어느정도 포기하고 들어갑니다. (여유가 되신다면 정말 대규모 상황에서는 어떨지도 고민해보시면 좋을 것 같긴합니다. 다만, 제가 알기로 국내 기준 단순 Redis만으로 처리가 어려울 정도의 많은 인원이 한번에 몰리는 경우는 제가 근무하고 있는 NOL 티켓에서 예매받는 BTS, 싸이 공연 정도가 유이한것으로 알고 있습니다.)

waiting_queue -> active_users로 전환시키는 구조도 대부분의 상황에서 무난한 판단이라고 보실 수 있어요.
다음 주차에 배울 이벤트를 통해 큐를 전환시키고, 클라이언트와의 통신을 웹소켓 등의 전이중 통신으로 한다면 완전 실시간 진입을 구현할 수는 있지만 지금은 폴링으로 구현하는 것 정도로 충분합니다. (한번에 모든것을 하려면 너무 힘드니 차근차근 단계를 밟아 나가면서 나중에 더 고도화 해봐요!)
다만 유저가 자신을 active_users로 갈아타게 해달라고 Client-Side에서 지속적으로 요청하면 부하가 너무 심해질 것이기 때문에 
🟠waiting_queue -> active_users 전환하는 폴링 로직은 서버에서 돌고 
유저는 자신이 전환이 됐는지를 확인만 하는 방식으로 구현하셔야합니다.
-> 근데 서버가 상태를 갖는것보다는 클라에서 폴링오는게 나을듯.
서버가 인메모리로 폴링하는건 훨씬 빠르고 주기도 짧아도 되지만 그 연결을 유지하고있어야하는거라?

-------------------------------------------------------------------
🥬🥬🥬
### 2. 콘서트 예약 시스템 빠른 매진 랭킹 설계 궁금한점 있습니다.

인터파크 사이트처럼 예매율로 인기 랭킹을 매겨보고 싶은데요,

이 경우 
레디스에 실시간으로 예매율을 기록해두고 db에 스냅샷을 뜨면 좋을지,
db 데이터를 주기적으로 분석해서 레디스로 가져오는것이 좋을지
데이터의 방향성이 주로 어떠한지 궁금합니다.

그리고 레디스 데이터를 db로 스냅샷을 뜬다하면 
그 db가 보통 RDBMS가 아니라 분석용 데이터 플랫폼일까요? 
과제에서는 mysql 테이블을 추가하면 될듯한데, 실무에서는 어떤식으로 배치성 집계 데이터를 저장하는지 궁금합니다. 

(과제와 관련성이 적을 수는 있지만 생각을 넓혀볼 기회인것같아 질문드려봅니다.! )

-------------------------------------------------------------------
(정리)
유저 페이지 조회 이벤트 -> 
1. Kafka 스트림에 발행 -> 
2. ETL:이벤트/로그를 가공하여 DW에 적재(or Data lake) -> 
3. DW 데이터를 AWS Athena 등 빅데이터 쿼리엔진으로 불러옴 ->
주기적으로 DW에서 끌어온 데이터를 Redis에 캐싱

어쨌든 데이터가 너무 크면 집계는 DW에서. 그걸 RDB로 넣어주는식.
백엔드팀이 DW에 능숙하다면 DW에 넣어주고.

방향성은 RDB(데이터가 적은경우) or DW 데이터(큰 경우)를 주기적으로 레디스로 캐싱.
게임만큼 실시간이 중요한건 아니라서. 

---
집계해야하는 데이터량이 적은 경우에는 그냥 단순히 RDB에 있는 데이터를 일정 시간마다 집계하여 레디스에 담아 퍼나르는 것이 효율적이구요.

처리해야하는 데이터량이 정말 많은 대기업 기준으로 어떤 프로세스를 타는지가 궁금하신것 같아서 이부분은 저희 주제에서 조금 벗어날 수 있지만 간략하게 설명드려보겠습니다!

우선 집계해야하는 원본 데이터의 량이 수백기가~페타바이트 급으로 가는 경우에는 RDB나 레디스에서 이 데이터를 다뤄낼 수가 없기 때문에 DW(데이터웨어하우스)라는 별도의 빅데이터용 저장소를 다루게 됩니다.
AWS의 Athena, GCP의 BigQuery가 이와 관련된 대표적인 서비스인데요. 이 아래에 있는 Hive, Spark, Hadoop, Flink 등의 컴포넌트는 백엔드 개발자가 직접 다룰일이 거의 없다고 봐도 무방합니다. 보통 '데이터 엔지니어'라는 분들이 관리하는 영역입니다. (그래도 궁금하시고 여유가 되신다면 한번 어떤 원리로 많은 양의 데이터를 처리할 수 있는지는 찾아보는것도 좋겠죠? 데이터 엔지니어링은 'MapReduce'라는 키워드로 시작하시는게 무난합니다 ㅎㅎ)

우선 집계를 위해 다뤄야하는 원본 데이터가 가장 클 가능성이 높은 PV(페이지 조회) 랭킹을 구현한다고 가정해보겠습니다.

유저가 상품 상세 페이지를 조회한 경우 이 사실을 RDB나 레디스에 담는게 아니라, Kafka나 AWS Kinesis와 같은 스트림에 이벤트를 발행합니다. (보통 이런 데이터를 로그 데이터라고 해요)
이 이벤트를 데이터 엔지니어 분들이 만든 데이터 플랫폼에서 받아서 적절히 처리하여 비즈니스에서 사용 가능한 형태로 정리하여 DW에 쌓는 프로세스가 돌게됩니다. 로그 데이터를 받아서 데이터를 이쁘게 편집하고, 필요한 내용을 바탕으로 집계를 돌려서 비즈니스 레벨에서 원하는 형태로 정리해주는걸 ETL이라고 표현합니다.
2-1. 실제 데이터 플랫폼에서 이벤트를 받아 로그 데이터를 쌓는 곳은 데이터의 규모나 종류에 따라 다릅니다. 원본 로그 자체도 DW에 쌓는 경우도 있고, 규모가 매우 큰 경우 DW로도 감당이 어렵기 때문에 그 보다 더 많은 데이터를 다루는것을 상정한 시스템인 (당연히 그만큼 제약사항도 많아지겠죠?) Data Lake에 쌓아두는 케이스도 있습니다. 그러나 저희 입장에서는 이 프로세스는 크게 중요한건 아니고 그렇게 해서 이쁘게 정리된 랭킹 정보가 DW에 있다는 사실이 중요한거죠.
DW에 있는 데이터를 AWS의 Athena와 같은 빅데이터 쿼리 엔진으로 불러옵니다. (사용법은 그냥 RDB에 SQL 날리는거랑 비슷해요. 다만 MySQL이나 Oracle과 같은 RDB에서 지원하는 SQL과 지원하는 함수에 차이가 약간 있습니다.)
이렇게 불러온 데이터는 이미 다 정리되고 필요한 정보만 뽑아온 데이터라 용량 문제가 해결되어서 그냥 메모리에서 다룰 수 있게됩니다.
DW에 쿼리하는것이 상당히 고비용의 작업이라 유저가 랭킹 정보를 요청할 때마다 계속 쿼리하는게 아니고, 이렇게 한번 DW에서 끌어온 데이터는 Redis같은데 담아서 캐싱하여 사용합니다.

+ 예매 정보같이 이미 RDB에 데이터가 저장되어있는데, 저장까지는 큰 문제 없으나 이걸 집계하는데 곤란한 상황이라면 (수천만건 이상의 데이터를 줄세우는 작업을 RDB안에서 돌리면 RDB에 가해지는 부하가 너무 심해서 안됩니다.) 데이터 플랫폼에서 RDB에 있는 데이터를 통째로 떠가서 집계한다음에 집계한 통계 데이터를 RDB에 다시 넣어주거나 DW로 제공합니다.
(당연히 데이터 플랫폼측에서 정리된 데이터를 어디에 던져줄지는 통계 데이터의 규모나 조직의 상황에 따라 달라집니다. 데이터량이 많고 백엔드팀이 DW에 쿼리하는 것에 능숙하다면 DW에 넣어줄 수도 있고, 만약 백엔드팀이 빅데이터에 대한 이해도가 낮고 관리 포인트를 늘리기 어려운 상황이라면 데이터 플랫폼 측에서 통계 데이터를 직접 RDB에 넣어줄 수도 있습니다.)

핵심만 다시 말씀 드리면 아래와 같이 정리될 것 같네요.
대규모 데이터를 편집해서 집계하는것을 전담으로 연구하는 '데이터 엔지니어'라는 분들이 있다.
데이터 량이 너무 많아서 RDB에 저장하는것 조차 부담이 된다면 저장하는 프로세스부터 그냥 데이터 플랫폼에서 처리한다.
이러한 사항을 다루는 분야를 보통 '데이터 엔지니어링' 혹은 '빅데이터 프로세싱'이라고 한다.


얘기가 나온김에 여담으로 재밌는 얘기를 한가지 드리면, 이렇게 뒷단에서 데이터를 처리하는 데이터 엔지니어 분들이나 이 데이터를 가지고 여러가지 실험들을 해보는 AI 엔지니어, 데이터 분석가 분들은 저희 같은 백엔드 엔지니어들을 '프론트엔드'라고 부릅니다.
전체 시스템 관점에서는 비즈니스 로직과 맞닿아있는 저희와 프론트엔드 개발자분들을 '서비스의 기능을 직접적으로 개발한다' 라는 공통점으로 그냥 프론트엔드로 퉁쳐서 생각하는거죠.
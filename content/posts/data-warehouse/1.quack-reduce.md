+++
title = 'quack-reduce'
date = 2024-06-30T00:20:22+09:00
draft = true
+++

## DuckDB 문서
주로 쓰는건 몇개 없음.
파이썬 API보다 Parquet Files가 훨씬 중요.

### python API
- duckdb.connect()
- .sql() 또는 .read_parquet()
- Result Conversion
	- fetchall()
	- pl() : 판다스 df()보다 빠름

### Parquet Files
- hive_partitioning
- union_by_name

### Hive Partitioning
- 실제로 query-server에서 읽을때 이걸 쓰고있어서 중요함.

하이브 파티셔닝은 읽어주는 전략 중 하나.
yar, month가 주요 키로 인식되어-
칼럼에 date가 없는 경우에도 키처럼 검색이 가능함.

이런식으로 특정 폴더(파티션) 데이터만 읽을 수 있음!
```sql
SELECT *
FROM read_parquet('orders/year=2021/*/*.parquet', hive_partitioning = true);
```

### Reading and Writing Parquet
읽는 방법 위주로 잘 본다
+ Metadata
-> 관련해서 Ice berg
: 하이브 파티셔닝처럼 모든 폴더를 읽어야할 필요가 있을까.
그런점에서 Ice berg는 메타데이터를 따로 들고있는.

(참고)
하이브 파티셔닝은 테이블을 partition keys 기반으로 여러 파일로 쪼갠다.
파일들은 폴더로 정리되고, 각 폴더안에 partition key는 폴더명에 의해 정해진다.



`union_by_name`


---
#### 해본 것
Makefile 실행
- run_me_first.py: 데이터를 다운받아 S3에 파일을 업로드
- quack.py: 람다로 duckdb 쿼리를 실행

로컬에서 DuckDB 실행
1. S3 파일에 로컬에서 접근하기
- Extensions / httpfs (HTTP and S3) / S3 API Support 	

```
CREATE SECRET ~
select * from 's3://{name}/dashboard/my_view.parquet';
```

2. partitioned 각각을 limit 10으로 가져오기



---
(참고)

DuckDB를 사용하면 메모리 상에 데이터베이스를 생성.
이 데이터베이스는 duckdb 모듈 내부에서 관리되며, 이 모듈을 사용하는 모든 코드에서 접근할 수 있습니다. 즉, 한 번 duckdb.sql 명령을 통해 쿼리를 실행하면, 그 쿼리의 결과를 다른 쿼리에서 참조할 수 있고, 데이터베이스는 디스크가 아닌 메모리에 저장되어 있어서 빠름
